--- 
title: "Analysis of stocks discussed in Reddit r/wallstreetbets"
author: "Xingyu Lu, Yue Xiong ,Yue Zhang"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---
```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Introduction


Background:
In January, 2021, there is a subreddit called r/wallstreetbets where participants discuss stocks and option trading. Previously, there are not much redditers discussing in this subreddit group. Only until January 2021, there was one person started to share his investment in `GameStop`, with the ticker to be `GME`, and explained his rationale. This posts started to get the public attention and also, the redditers discovered that `GME` has been over short by some major financial institute, i.e. short interest is greater than 100%. This attracted more and more people into the r/wallstreetbets. The newly joined people are mainly young retailer traders who have little background in investment knowledge and risk management. They were not satisfied with the over short on some stocks by major Wall Street companies, so they started to buy those stocks, mostly `GME`. They were especially angered by the Citron Research's comment, which predicted the `GME`'s stock price would decrease continuously. This actually initiated a short squeeze on `GME`, pushing their stock prices up significantly. After a few days, Elon Musk also tweeted out a link to the r/wallstreetbets subreddit, which further enlightened the redditers and broadcasted to even more people all over the world. Later on, the official r/wallstreetbets Discord server was banned due to the "hateful and discriminatory content" and Robinhood started to restricted the trade of heavily shorted stocks such as `GME`, `AMC Entertainment Holdings Inc` (`AMC`), `BlackBerry Limited` (`BB`), `Nokia` (`NOK`), and etc. Other trading platform also followed Robinhood to restrict the trade of those stocks, which were TD Ameritrade, E-Trade, and Webull respectively. 

There were 2 waves of major short squeeze led by the participants in this subreddit, which were around January/Feburary and May/June period. Also, the focus of the 2 waves were different, while for the 1st wave, `GME` is the focus stock, `AMC` overtaken `GME` became the focus stock in the 2nd wave.

In this project, we would like to collect stock related data, Reddit related data (mainly posts) as well as the Google search related data to generate visualization and get some useful insights. We'll be trying to answer following questions through this study:

1. How big is the impact of reddit wallstreetbet? I.e. no. of news, downloads of Robinhood and other trading apps,

2. For each targeted stock, what’s the relationship between reddit posts and stock price/volume etc.

3. Why does Wallstreetbet choose these target stocks? What’re their similarities? How do their choices change as time evolves? Gamestop, AMC, Blueberry, Nokia, SPCE, PLTR, TSLA.

In the following chapters, we will use data source to plot graphs, trying to answer the above questions. Also, we would like to explore other aspects from the graphs and come up with more conclusions.

For more details of this project, click the link here or copy the url https://github.com/stellazhangyue/redditWSB.git and open it in browser to go to our Github repository and navigate code in .Rmd files.

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Data sources

To answer the 3 questions mentioned in Chapter 1 - Introduction, we need to collect the following data:

1. Stock related data: this data mainly contains the price and volume of the stocks that are discussed in r/wallstreetbets subreddit, which help to indicate the stock performance. This data can be found from [*Yahoo Finance*](https://finance.yahoo.com/), which provides financial news, data and commentary including stock quotes, press releases, financial reports, and original content.

2. Reddit related data: this data mainly contains the posts information in r/wallstreetbets subreddit, which helps to extract out the information about the stocks discussed and the post attitude. This data can be found from [*Reddit API*](https://www.reddit.com/dev/api/). Reddit API is the official portal Reddit created to facilitate developers in their app-building endeavors, where developers can retrieve the constantly updating feeds of Reddit posts.

3. Google search related data: this data mainly contains the website search ans news search information in Google. Since r/wallstreetbets subreddit, the heavily shorted stocks as well as the disagreement between retailer traders and professional traders were relatively new to the general public, we assume that they would actively search in Google to follow this event. Therefore, using Google search data can also serve as another indicator to show the public attention. This data can be found from [*Google Trends*](https://trends.google.com/trends/?geo=US). Google Trends is a website by Google that analyzes the popularity of top search queries in Google Search across various regions and languages. 


```{r}
top_10_tickers <- c('GME','AMC','BB','NOK','SND','NAKD','PLTR','CLOV','RETA','MAR','^GSPC')
get_price_df <- function(ticker) {
  GME_S <- getSymbols(ticker, src="yahoo", from = "2020-01-01", auto.assign = FALSE)
  gme_sp_df <- data.frame(Date=index(GME_S),coredata(GME_S))
  gme_sp_df <- gme_sp_df %>%
    select(Date, ends_with("Close"), ends_with("Volume")) %>%
    rename(Close = ends_with("Close")) %>%
    rename(Volume = ends_with("Volume"))
  gme_sp_df$Symbol = ticker
  gme_sp_df <- select(gme_sp_df, Symbol, Date, Close, Volume)
  return(gme_sp_df)
}
df_output = data.frame()
for(i in top_10_tickers) {
  tmp <- get_price_df(i)
  df_output <- rbind(df_output, tmp)
}
write.csv(df_output,'data/tmp/top_10_stocks_prices.csv',row.names=FALSE)
```

```{r}
get_price_df <- function(ticker) {
  df <- getSymbols(ticker, src="yahoo", from = "2020-01-01", auto.assign = FALSE)
  daily_return <- na.omit(periodReturn(df, period="daily", type = "arithmetic"))
  dat <- data.frame(date=index(daily_return), daily_return$daily.returns)
  dat$daily.returns <- dat$daily.returns * 100
  dat <- dat %>%
    rename(Daily_return = daily.returns, Date = date)
  dat$Symbol = ticker
  dat <- select(dat, Symbol, Date, Daily_return)
  return(dat)
}
df_output = data.frame()
for(i in top_10_tickers) {
  tmp <- get_price_df(i)
  df_output <- rbind(df_output, tmp)
}
write.csv(df_output,'data/clean/stock/top_10_stocks_daily_return.csv',row.names=FALSE)
```

<!--chapter:end:02-data.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Data transformation

```{r, message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(tm)
library(tidytext)
library(textdata)

source("sentiment_analysis.R")
```


## Google Trends Data Transformation
In the Google Trends source data, it assigns a number to each keyword for each period(week). The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term. A score of <1 means there is some search interest but it's extremely low. We are replacing all <1's with 0.5, so that we can keep the data frame consistent with all scores as numeric data. 

We also need to tidy the dataset because some of the column names are not names of variables, but values of a variable: the original column names represent the values of the `keyword` variable and the values in the columns represents the values of `score`, and each row represents 5 observations, not one. To tidy the dataset, we make the offending columns into a new pair of variables using `pivot_longer`. 
```{r echo=FALSE}
combined_web_raw <- readr::read_csv("data/raw/gtrends/combined_web.csv")

combined_web_clean <- combined_web_raw %>%
  mutate(`r/WallStreetBets` = replace(`r/WallStreetBets`, `r/WallStreetBets` == "<1", 0.5)) %>%
  mutate(`r/WallStreetBets` = as.numeric(`r/WallStreetBets`)) %>%
  pivot_longer(cols = !Week,
             names_to = "keyword",
             values_to = "score"
             )
write.csv(combined_web_clean, file = ("data/clean/gtrends/combined_web.csv"),row.names=FALSE)

combined_news_raw <- readr::read_csv("data/raw/gtrends/combined_news.csv")
combined_news_clean <- combined_news_raw %>%
  mutate(`r/WallStreetBets` = replace(`r/WallStreetBets`, `r/WallStreetBets` == "<1", 0.5),
         `GameStop` = replace(`GameStop`, `GameStop` == "<1", 0.5)) %>%
  mutate(`r/WallStreetBets` = as.numeric(`r/WallStreetBets`),
         `GameStop` = as.numeric(`GameStop`)) %>%
  pivot_longer(cols = !Week,
             names_to = "keyword",
             values_to = "score"
             )
write.csv(combined_news_clean, file = ("data/clean/gtrends/combined_news.csv"),row.names=FALSE)
```

After cleaning, it contains 3 rows and 260 records.
```{r}
knitr::kable(data.frame(
                cols = c('week','keyword','score'),
                data_type = c('date', 'string', 'numeric'),
                description = c("Which week?","Search keyword","Search interest score")
              ), 
             col.names = c('Column', 'Type', 'Description'),
             row.names = F,font_size = 10)
```


## Reddit Data Transformation
```{r echo=FALSE}
reddit_filenames <- list.files("data/raw/reddit/daywise", pattern="*.csv", full.names=TRUE)
reddit_df <- do.call(rbind,lapply(reddit_filenames, read.csv))
reddit_df <- subset(reddit_df, select = -c(X))
reddit_df <- reddit_df[!duplicated(reddit_df),]
```

### Parse Stock tickers from Reddit posts
There were 2 waves of sharp stock price change for WallStreetBets (WSBs) stocks in 2021, which were around January/Feburary and May/June period. These 2 waves are mainly caused by the discussion in the sub-Reddit group, WallStreetBets. Therefore, we have downloaded the Reddit posts that have been posted during these 2 periods and also extend some time before and after the wave. This is because we believe it takes some time for the initial posts to get public attention and then the posts starts to influence the stock price. Also, by including the posts that are slightly after the wave, we can do a comparison during and after the wave to check how big the effects of the posts in the sub-Reddit group on the stock price. Therefore, we have downloaded the data for the whole January, February, May, June and July, which have completely cover the 2 waves and also cover some extension. Also, we have parsed the posts on daily basis, which also helps to show the change over time.

Before starting to do the analysis, we need to find the list of WSB stocks first. the Generally speaking, all the stocks that have been discussed in the WallStreetBets sub-Reddit group should be considered as WSB stock. However, given the time frame that we considered is relatively long, which is 5 months, also, the WSBs has caught huge public attention, which also attracted a lot of new redditers. Therefore, there are many people participate in the discussion and the stocks mentioned are quite diversified. In this project, we will only focus on the top 10 stocks that have been mentioned in the posts. 

In the source data, which are the daily posts csv files, there are 2 columns that may contains the stock information, which are `title` and `selftext` respectively. Therefore, we need to first find the stock ticker from these 2 columns. Generally, the stock ticker should start with `$` and followed by 2 to 4 consecutive upper case letters. However, in the source data, there is no `$` exists, we cannot get the exact stock data but rather, we can extract out the list of possible stock tickers by extracting all the sub-strings which are consists of 2 to 4 consecutive upper case letters. 

We have used regex in python to extract out the list of possible stock tickers from `title` and `selftext` respectively for each of the daily reddit csv and then we combine these 2 lists together. However, there might be the case that the possible stock tickers are mentioned by the redditers both in `title` and `selftext`. In this case, we define that the possible stock ticker should be considered only once. Therefore, after combining the 2 lists together, we also need to drop duplicates to make sure there is no double count of possible stock ticker from both `title` and `selftext`. With that, we can make sure that the count of every possible stock ticker in 1 row is at most 1. 

Then, we start to count the possible stock ticker and summarize the count information into a dictionary. For every daily reddit csv, we will get a possible stock ticker count dictionary and combine them together to get the final count dictionary and stored it into the clean data folder. 

Now, in r, we can read the file which contains the information of the final possible stock ticker count. Since we are only interested in the top 10 WSB stocks, we need to re-order the data frame according to the word count in descending order. 

We noticed that there are some invalid tickers exists in the data frame, but that is expected since there is no perfect way to extract out the stock ticker due to the lack of `$` in the data source, so we manually go through the data frame and select out the top 10 tickers, which are: `GME`, `AMC`, `BB`,`NOK`, `SND`, `NAKD`, `PLTR`, `CLOV`, `RETA`, `MAR`, respectively.

```{r}
df <- read.csv('data/clean/reddit/possible_tickers_count.csv')
df %>% arrange(desc(count)) -> df
write.csv(df,"data/clean/reddit/ranked_possible_tickers_count.csv", row.names = FALSE)
```

### Subreddit overview
```{r}
reddit_overview_df <- reddit_df %>%
  mutate(date_utc = as.Date(as.POSIXct(as.numeric(created_utc), origin="1970-01-01")),
         contains_gme = grepl("GME", title, ignore.case = TRUE) | grepl("GME", selftext, ignore.case = TRUE),
         contains_amc = grepl("AMC", title, ignore.case = TRUE) | grepl("AMC", selftext, ignore.case = TRUE)) %>%
  select(date_utc, author, postid, score, contains_amc, contains_gme) %>%
  group_by(date_utc) %>%
  summarise(post_cnt = n(),
            distinct_user_cnt = n_distinct(author),
            gme_cnt = sum(contains_gme),
            amc_cnt = sum(contains_amc))

date_utc <- seq(as.Date("2021/3/1"), as.Date("2021/4/30"), "days")
empty_df <- data.frame(date_utc)
empty_df$post_cnt <- NA
empty_df$distinct_user_cnt <- NA
empty_df$gme_cnt <- NA
empty_df$amc_cnt <- NA

reddit_overview_df <- rbind(reddit_overview_df, empty_df)

write.csv(reddit_overview_df,"data/clean/reddit/wsb_subreddit_overview.csv", row.names = FALSE)
```


### Frequent keywords in Reddit posts
```{r warning=FALSE}
get_reddit_wc_df <- function(start_dt, end_dt) {
  reddit_wc <- reddit_df %>%
    filter(score >= 10) %>%
    mutate(date_utc = as.Date(as.POSIXct(as.numeric(created_utc), origin="1970-01-01")),
           text = cleanData(str_trim(paste(title, selftext)))) %>%
    filter(date_utc <= as.Date(end_dt) & date_utc >= as.Date(start_dt)) %>%
    select(date_utc, text)
  
  reddit_corpus <- Corpus(VectorSource(reddit_wc$text), readerControl = list(reader = readPlain, language = "en_US", load = TRUE))
    
  #Clean up the corpus
  reddit_corpus <- tm_map(reddit_corpus, removePunctuation)
  reddit_corpus <- tm_map(reddit_corpus, removeNumbers)
  reddit_corpus <- tm_map(reddit_corpus, stripWhitespace)
  reddit_corpus <- tm_map(reddit_corpus, removeWords, stopwords("english"))
  reddit_corpus <- tm_map(reddit_corpus, removeWords, c("http", "www", "amp", "https", "the", "com"))
  
  #Generate Document Term Matrix
  reddit_tdm <- TermDocumentMatrix(reddit_corpus)
  reddit_matrix <- as.matrix(reddit_tdm) 
  reddit_words <- sort(rowSums(reddit_matrix),decreasing=TRUE) 
  reddit_wc_df <- data.frame(word = names(reddit_words),freq=reddit_words)
  
  #Get stop words from 'tidytext' package and remove from data frame
  lexStopWords <- stop_words
  reddit_wc_df <- reddit_wc_df %>% 
    anti_join(lexStopWords, by = c("word" = "word")) %>% 
    filter(!word  %in% c("http", "www", "amp", "https", "the", "com", "removed", "deleted", "don", "png"))
  
  return(reddit_wc_df)
}

reddit_wc_df_1 <- get_reddit_wc_df('2021-01-01', '2021-01-31')
reddit_wc_df_2 <- get_reddit_wc_df('2021-02-01', '2021-02-28')
reddit_wc_df_567 <- get_reddit_wc_df('2021-05-01', '2021-07-31')
reddit_wc_df_total <- rbind(reddit_wc_df_1, reddit_wc_df_2, reddit_wc_df_567) %>%
  group_by(word) %>%
  summarise(freq = sum(freq)) %>%
  arrange(desc(freq))

write.csv(reddit_wc_df_total, file = ("data/clean/reddit/reddit_wc_total.csv"), row.names=FALSE)
```



### Sentiment Analysis on Reddit Posts regarding GME

```{r}
# filter posts with score > 10 and contains GME in title or text
reddit_GME_sa <- reddit_df %>%
  filter(score >= 10) %>%
  select(postid, created_utc, title, selftext) %>%
  mutate(title_contains_GME = grepl("GME", title, ignore.case = TRUE),
         text_contains_GME = grepl("GME", selftext, ignore.case = TRUE)) %>%
  filter(title_contains_GME | text_contains_GME) %>%
  mutate(date_utc = as.Date(as.POSIXct(as.numeric(created_utc), origin="1970-01-01")),
         text = cleanData(str_trim(paste(title, selftext))),
         postid = paste(postid, date_utc, sep = "_"))
```

```{r warning=FALSE}
reddit_GME_sa_Jan <- reddit_GME_sa %>%
  filter(date_utc <= as.Date('2021-01-31') & date_utc >= as.Date('2021-01-01')) %>%
  select(postid, text)

gme_nrc_Jan <- get_nrc_sentiments(reddit_GME_sa_Jan)
```

```{r warning=FALSE}
reddit_GME_sa_Feb <- reddit_GME_sa %>%
  filter(date_utc <= as.Date('2021-02-28') & date_utc >= as.Date('2021-02-01')) %>%
  select(postid, text)

gme_nrc_Feb <- get_nrc_sentiments(reddit_GME_sa_Feb)
```

```{r warning=FALSE}
reddit_GME_sa_567 <- reddit_GME_sa %>%
  filter(date_utc <= as.Date('2021-07-31') & date_utc >= as.Date('2021-05-01')) %>%
  select(postid, text)

gme_nrc_567 <- get_nrc_sentiments(reddit_GME_sa_567)
```

```{r}
gme_nrc_Jan_agg <- gme_nrc_Jan %>%
  select(textDate, sentiment, wordCount) %>%
  group_by(textDate, sentiment) %>%
  dplyr::summarise(wordCount = sum(wordCount)) 

gme_nrc_Feb_agg <- gme_nrc_Feb %>%
  select(textDate, sentiment, wordCount) %>%
  group_by(textDate, sentiment) %>%
  dplyr::summarise(wordCount = sum(wordCount)) 

gme_nrc_first_wave <- rbind(gme_nrc_Jan_agg, gme_nrc_Feb_agg)

write.csv(gme_nrc_first_wave, file = ("data/clean/reddit/gme_sa_nrc_first_wave.csv"), row.names=FALSE)
``` 


```{r}
gme_nrc_567_agg <- gme_nrc_567 %>%
  select(textDate, sentiment, wordCount) %>%
  group_by(textDate, sentiment) %>%
  dplyr::summarise(wordCount = sum(wordCount)) 

write.csv(gme_nrc_567_agg, file = ("data/clean/reddit/gme_sa_nrc_second_wave.csv"), row.names=FALSE)
```

<!--chapter:end:03-cleaning.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Missing values
```{r include=FALSE}
library(tidyverse)
library(patchwork)
library(visdat)
library(ggplot2)
library(naniar)

source("plot_missing.R")
```

## Reddit post dataset
We first create the missing values plots, both count and percent, for the reddit post dataset.
```{r echo=FALSE}
filenames <- list.files("data/raw/reddit/daywise", pattern="*.csv", full.names=TRUE)
df <- do.call(rbind,lapply(filenames, read.csv))
df <- subset(df, select = -c(X))
df <- df[!duplicated(df),]
```

```{r echo=FALSE}
mp_df <- df %>% 
  replace_with_na(replace = list(author = "", selftext=c(" removed ", " deleted ")))

plot_missing_patterns(mp_df, FALSE)
plot_missing_patterns(mp_df, TRUE)
```

We can observe from the missing value plots that there are 4 different missing patterns, which are complete cases (47.19%), missing `selftext`  (45.08%), missing both `selftext` and `author` (4.77%), and missing `author` only (2.95%). Therefore, out of the 7 variables in the dataset, only 2 variables might be missing, which are `selftext` and `author` respectively. Among the 2 variables, 49.85% of the `selftext` is missing while only 7.72% of the `author` is missing.

Looking into the `selftext` column in the original dataset, we noticed that there are 3 major levels that account for ~90% of the data, which are "removed", "" and "deleted". Consulted the reddit documents, we found that "removed" means the post is either removed by the moderators of the subreddit group or the administrator; "deleted" means the post is deleted by the content author; "" appears when a post doesn't have body text or a post's body contains pictures only. In this regard, we are only considering "removed" and "deleted" as NA, ""s are considered as valid inputs. About 45% of the posts are "removed", which means the level of censorship is relatively high. 

```{r echo=FALSE}
body_agg <- df %>%
  mutate(selftext = trimws(selftext)) %>%
  group_by(selftext) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  mutate(percent = round(count/sum(count)*100, 2)) %>%
  top_n(20)

body_agg
```

Looking at the `author` column, 7.72% of the posts are missing `author`.  We noticed that 61.76% of the posts that don't have an `author` are also missing `selftext`. 
```{r echo=FALSE}
author_agg <- mp_df %>%
  mutate(selftext = trimws(selftext),
         author = trimws(author)) %>%
  filter(is.na(author)) %>%
  group_by(selftext) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  mutate(percent = round(count/sum(count)*100, 2)) %>%
  top_n(20)

author_agg
```

These missing values wouldn't impact our analysis, because we are looking at the data at the aggregate level. Our analysis only cares about the change of the count of the posts in particular period of time. Even though those posts are deleted or removed in a later time, we should still include them in our analysis.

## Google Trend dataset
We created the missing values plots, both count and percent, for two Google trend datasets and found there's no missing value in both datasets
```{r echo=FALSE}
gt_web <- read_csv("data/raw/gtrends/combined_web.csv")
plot_missing_patterns(gt_web, FALSE)
plot_missing_patterns(gt_web, TRUE)
```

```{r echo=FALSE}
gt_news <- read_csv("data/raw/gtrends/combined_news.csv")
plot_missing_patterns(gt_news, FALSE)
plot_missing_patterns(gt_news, TRUE)
```


<!--chapter:end:04-missing.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Results
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(quantmod)
library(PerformanceAnalytics)
library(ggplot2)
library(plyr)
library(scales)
library(ggcorrplot)
library(reshape2)
library(plotly)
library(patchwork)
library(maps)
library(mapdata)
library(lubridate)
library(treemap)
library(wordcloud2)
library(highcharter)
```

```{r}
light_blue <- '#88A8F2'
light_red <- '#EF856E'
green <- "#8DF186"
purple <- "#D5A7F1"
```

## What's the impact of r/wallstreetbet?

## Stock Data and Information
### Stock Data Plot
The most representative WSB stocks are `GME` and `AMC`. Also, these 2 stocks are the top 2 most widely discussed stocks in WallStreetBet sub-reddit analysed in Chapter 3 - cleaning. Therefore, we choose these 2 stocks and show the stock price and volume change in the past one year.

The graph is interactive, where we can choose select the period of time that we want to focus on by dragging on the bottom graph. This allows us to zoom in and see the graph clearly. Also, we can choose the specific period, i.e. the past 1 year or past 6 months data by pressing the corresponding buttons above and we can easily go back to the main graph by pressing the "RESET" botton.

```{r}
getSymbols("GME",src='yahoo')
df_GME <- data.frame(Date=index(GME),coredata(GME))

# Generate Bollinger Bands
bbands <- BBands(GME[,c("GME.High","GME.Low","GME.Close")])

df_GME <- subset(cbind(df_GME, data.frame(bbands[,1:3])), Date >= "2020-12-01")

for (i in 1:length(df_GME[,1])) {
  if (df_GME$GME.Close[i] >= df_GME$GME.Open[i]) {
      df_GME$direction[i] = 'Increasing'
  } else {
      df_GME$direction[i] = 'Decreasing'
  }
}
i <- list(line = list(color = 'green'))
d <- list(line = list(color = 'red'))

# candlestick chart
fig <- df_GME %>% plot_ly(x = ~Date, type="candlestick",
          open = ~GME.Open, close = ~GME.Close,
          high = ~GME.High, low = ~GME.Low, name = "GME",
          increasing = i, decreasing = d)
fig <- fig %>% add_lines(x = ~Date, y = ~up , name = "Bollinger Bands",
            line = list(color = '#ccc', width = 0.5),
            legendgroup = "Bollinger Bands",
            hoverinfo = "none", inherit = F) 
fig <- fig %>% add_lines(x = ~Date, y = ~dn, name = "Bollinger Bands",
            line = list(color = '#ccc', width = 0.5),
            legendgroup = "Bollinger Bands", inherit = F,
            showlegend = FALSE, hoverinfo = "none") 
fig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = "Moving Avg",
            line = list(color = '#E377C2', width = 0.5),
            hoverinfo = "none", inherit = F) 
fig <- fig %>% layout(yaxis = list(title = "Price"))

# volume chart
fig2 <- df_GME
fig2 <- fig2 %>% plot_ly(x=~Date, y=~GME.Volume, type='bar', name = "GME Volume",
          color = ~direction, colors = c('green','red')) 
fig2 <- fig2 %>% layout(yaxis = list(title = "Volume"))

# date slider 
rs <- list(visible = TRUE, x = 0.5, y = -0.055,
           xanchor = 'center', yref = 'paper',
           font = list(size = 9),
           buttons = list(
             list(count=1,
                  label='RESET',
                  step='all'),
             list(count=1,
                  label='1 YR',
                  step='year',
                  stepmode='backward'),
             list(count=6,
                  label='6 MO',
                  step='month',
                  stepmode='backward'),
             list(count=3,
                  label='3 MO',
                  step='month',
                  stepmode='backward'),
             list(count=1,
                  label='1 MO',
                  step='month',
                  stepmode='backward')
           ))

# combined plot
fig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,
             shareX = TRUE, titleY = TRUE)
fig <- fig %>% layout(title = paste("GME: 2020-12-01 -",Sys.Date()),
         xaxis = list(rangeselector = rs),
         legend = list(orientation = 'h', x = 0.5, y = 1,
                       xanchor = 'center', yref = 'paper',
                       font = list(size = 10),
                       bgcolor = 'transparent'))

fig
```

```{r}
getSymbols("AMC",src='yahoo')
df_AMC <- data.frame(Date=index(AMC),coredata(AMC))

# Generate Bollinger Bands
bbands <- BBands(AMC[,c("AMC.High","AMC.Low","AMC.Close")])

df_AMC <- subset(cbind(df_AMC, data.frame(bbands[,1:3])), Date >= "2020-12-01")

for (i in 1:length(df_AMC[,1])) {
  if (df_AMC$AMC.Close[i] >= df_AMC$AMC.Open[i]) {
      df_AMC$direction[i] = 'Increasing'
  } else {
      df_AMC$direction[i] = 'Decreasing'
  }
}
i <- list(line = list(color = 'green'))
d <- list(line = list(color = 'red'))

# candlestick chart
fig3 <- df_AMC %>% plot_ly(x = ~Date, type="candlestick",
          open = ~AMC.Open, close = ~AMC.Close,
          high = ~AMC.High, low = ~AMC.Low, name = "AMC",
          increasing = i, decreasing = d)
fig3 <- fig3 %>% add_lines(x = ~Date, y = ~up , name = "Bollinger Bands",
            line = list(color = '#ccc', width = 0.5),
            legendgroup = "Bollinger Bands",
            hoverinfo = "none", inherit = F) 
fig3 <- fig3 %>% add_lines(x = ~Date, y = ~dn, name = "Bollinger Bands",
            line = list(color = '#ccc', width = 0.5),
            legendgroup = "Bollinger Bands", inherit = F,
            showlegend = FALSE, hoverinfo = "none") 
fig3 <- fig3 %>% add_lines(x = ~Date, y = ~mavg, name = "Moving Avg",
            line = list(color = '#E377C2', width = 0.5),
            hoverinfo = "none", inherit = F) 
fig3 <- fig3 %>% layout(yaxis = list(title = "Price"))

# volume chart
fig4 <- df_AMC
fig4 <- fig4 %>% plot_ly(x=~Date, y=~AMC.Volume, type='bar', name = "AMC Volume",
          color = ~direction, colors = c('green','red')) 
fig4 <- fig4 %>% layout(yaxis = list(title = "Volume"))

# date slider 
rs <- list(visible = TRUE, x = 0.5, y = -0.055,
           xanchor = 'center', yref = 'paper',
           font = list(size = 9),
           buttons = list(
             list(count=1,
                  label='RESET',
                  step='all'),
             list(count=1,
                  label='1 YR',
                  step='year',
                  stepmode='backward'),
             list(count=6,
                  label='6 MO',
                  step='month',
                  stepmode='backward'),
             list(count=3,
                  label='3 MO',
                  step='month',
                  stepmode='backward'),
             list(count=1,
                  label='1 MO',
                  step='month',
                  stepmode='backward')
           ))

# combined plot
fig3 <- subplot(fig3, fig4, heights = c(0.7,0.2), nrows=2,
             shareX = TRUE, titleY = TRUE)
fig3 <- fig3 %>% layout(title = paste("AMC: 2020-12-01 -",Sys.Date()),
         xaxis = list(rangeselector = rs),
         legend = list(orientation = 'h', x = 0.5, y = 1,
                       xanchor = 'center', yref = 'paper',
                       font = list(size = 10),
                       bgcolor = 'transparent'))

fig3
```

From the above 2 graphs, we can clearly see that there are 2 waves of the WSB stocks, which are indicated by the sudden spike of the stock price as well as the sudden increase of the volume. These 2 waves are around January - Feburary and May - June period respectively. These 2 graphs actually double confirm with our previous claims. 

Also, the gray and red line in the price graph is the Bollinger Bands, which is the price level at 1 standard deviation level above and below the sample moving average of the price, which is able to capture the volatility of the stock. Generally, Bollinger bands is used to check whether prices are relatively high or low as comparing to the historical price. Here, we can see that for the beginning of the 2 waves, the price of both `GME` and `AMC` break the Bollinger upper bands, which suggests that the price of these 2 stocks are extremely high as comparing to the historical data. However, the Bollinger bands also adjusted themselves quickly to accommodate the sudden price change, so we can see that the price of these 2 stocks fall back within the Bollinger bands after a short period of time.

### Stock Risk Assessment
While the previous 2 graphs mainly focus on the price and volume aspects, there is one more aspect that are very important to the stock, which is risk. Risk can be measured by the standard deviation. Here, we would like to explore how the risk of the 2 stocks change before and after the 2 waves, which are `GME` and `AMC` respectively. Thus, we need to calculate the moving standard deviation of the 2 stocks over time.

Here, we take the closing price to be the one calculating the moving standard deviation since closing price is the most representative price among all the 5 prices. For each month, it is about 4 weeks and the stock market only opens in weekdays, thus, there are about 4*5 = 20 days with stock data every month. Therefore, we set the moving window to be 20, which is roughly about 1 month time. 

Also, we take the data since the start of last year because we want to see whether there is any difference in terms of the risk of `GME` and `AMC` by comparing the moving standard deviation before and after the 2 waves.
```{r}
getSymbols("GME",src='yahoo')
df_GME <- data.frame(Date=index(GME),coredata(GME))

df_GME <- subset(df_GME, Date >= "2020-01-01")
df_GME['moving_std'] <- rollapply(df_GME['GME.Close'], width = 20, FUN = sd, na.pad = TRUE)
ggplot(df_GME, aes(x=Date, y=moving_std)) +
  geom_line() +
  xlab("Date") +
  ylab('Moving Standard Deviation') + 
  ggtitle('Change of risk of GME over time')
```

```{r}
getSymbols("AMC",src='yahoo')
df_AMC <- data.frame(Date=index(AMC),coredata(AMC))

df_AMC <- subset(df_AMC, Date >= "2020-01-01")
df_AMC['moving_std'] <- rollapply(df_AMC['AMC.Close'], width = 20, FUN = sd, na.pad = TRUE)
ggplot(df_AMC, aes(x=Date, y=moving_std)) +
  geom_line() +
  xlab("Date") +
  ylab('Moving Standard Deviation') + 
  ggtitle('Change of risk of AMC over time')
```

From the 2 graphs above, we have following observations:

1. Around Jan/Feb and May/June period, there are huge changes for both `GME` and `AMC` in terms of risk.

2. `GME` has relatively higher risk than  `AMC` as `GME`'s moving standard deviation reached more than 100 while the highest moving standard deviation of `AMC` is about 13.

3. The risk of `GME` is highest in the first wave during Jan/Feb period while the risk of `AMC` is highest in the second wave during May/Jun period. This can be supported by the general knowledge that `GME` is the leading stock in the 1st wave while `AMC` is the leading stock in the 2nd wave.

### Most Popular Stocks
Besides studying about the 2 most representative stocks in WSBs, which are `GME` and `AMC` respectively, we could also study some other WSB stocks, which are not as popular as `GME` and `AMC` but they are still very representative and have received a lot of public attention.

Therefore, we extend our study to the top 10 mentioned stocks in the posts of WallStreetBets sub-reddit group. These 10 stocks has been identified in the Chapter 3 - Cleaning, which are: `GME`, `AMC`, `BB`, `NOK`, `SND`, `NAKD`, `PLTR`, `CLOV`, `RETA`, `MAR`, respectively.

Now, we can plot bar chart to show the count of posts that have mentioned about the stock for each stock.

```{r}
df <- read.csv('data/clean/reddit/ranked_possible_tickers_count.csv')
top_10_tickers <- c('GME','AMC','BB','NOK','SND','NAKD','PLTR','CLOV','RETA','MAR')
df %>% filter(possible_ticker %in% top_10_tickers) %>%
    ggplot(aes(x = fct_reorder(possible_ticker, desc(count)), y = count)) +
    geom_bar(stat = "identity") +
    ggtitle("Number of Posts Mentioned the Stock Ticker") +
    xlab("Stock Ticker") + 
    ylab("Count")
```

```{r}
df %>% 
  filter(possible_ticker %in% top_10_tickers) %>%
  mutate(ticker=paste(possible_ticker, count, sep ="\n"))%>%
  treemap(index = c("ticker"), vSize = "count", vColor = "count", type = "value", title = "Ticker mentions count")
```

From the graph above, we can see that `GME` and `AMC` are the dominant 2 dominant stocks that were discussed in WallStreeBets sub-reddit, which appears in around `r round(df$count[1]/sum(df$count)*100, 4)`% and `r round(df$count[2]/sum(df$count)*100, 4)`% of the total posts respectively. However, ``BB`, `NOK`, `SND` and the other stocks have also received some public attention, which are worthwhile to include them in the future analysis to get a bigger and clearer picture regarding the WSB stocks.

### Time Series of the Top 10 Stocks
Now, we want to study how the top 10 stocks' prices move over the past one year. Again, since the closing price is the most representative price among all the 5 prices, we just use closing price to calculate the daily return change over time. The equation used to calculate the daily stock return is that: `(current closing price - previous closing price) / previous closing price * 100%`.

```{r}
df_price <- read.csv('data/clean/stock/top_10_stocks_daily_return.csv')
df_price %>%
  select(Date, Symbol, Daily_return) %>%
  filter(Date >= as.Date('2021-01-01')) %>%
  filter(Date <= as.Date('2021-09-01')) %>%
  filter(Symbol != "^GSPC") %>%
  pivot_wider(names_from = Symbol, values_from = Daily_return, values_fill = 0) %>%
  gather(key = "variable", value = "value", -Date) -> df_price

df_price$Date <- as.Date(df_price$Date)

ts1 <- df_price %>%
  hchart("line",hcaes(x = Date, y = value, 
                      group = variable)) %>%
  hc_chart(zoomType = "x") %>%
  hc_colors(c("steelBlue", "sienna", "seaGreen", "teal", "violet", 
                       "orange", "darkSlateGray", "darkOrchid", "darkRed", "coral", "crimson")) %>%
  hc_legend(align = "center", verticalAlign = "bottom",layout = "horizontal") %>%
  hc_xAxis(title = list(text = "Date"),
           labels = list(format = '{value:%b %d}')) %>%
  hc_yAxis(title = list(text = "Daily Return (%)"),
           tickInterval = 20,
           max = 400) %>%
  hc_title(text = "<b>Daily Return of Top 10 mentioned Stocks</b>") %>%
  hc_subtitle(text = "Click and drag in the plot area to zoom in on a time span") %>%
  hc_plotOptions(area = list(lineWidth = 0.5)) 

ts1
```

From the above graph, we can have the following observation:

1. `AMC` has the highest daily return in the end of January 2021, which was around 300% increase as comparing to the previous day price. 

2. `GME` and `AMC`'s price change start to fluctuate a lot since the mid of the January, which marks the beginning of the WSB 1st wave. During this period, they started to get public attention.

3. In 27th Jan, the 1st wave of WSB reaches the peak, where `GME`, `AMC`, `BB`, `NOK`, `NAKD` achieved their highest daily return change on the same day.

4. Generally, the change in daily return in the 1st WSB wave is higher than the change in daily return in the 2nd WSB wave. This occurs since the price of the stocks has increases a lot in the first wave already, so in terms of the return (%), it is very hard to have as big change as that in 1st wave.

5. The stocks are included in different waves, while`PLTR` is the stock that is only active in 1st wave, there are other stocks that are active only in the 2nd wave, which are `CLOV`, `SND`, `RETA` respectively. For `GME`, `AMC`, `BB`, `NOK`, `NAKD`, they are active in both of the waves. However, for `MAR`, its stock price does not change much through out the 2 waves.

### Correlations between top 10 mentioned stocks
We removed `CLOV` from the ticker for 2020, because it went public in 2021.
```{r}
top_10_tickers <- c('GME','AMC','BB','NOK','SND','NAKD','PLTR','CLOV','RETA','MAR')

# remove CLOV
top_10_tickers_2020 <- c('GME','AMC','BB','NOK','SND','NAKD','PLTR','RETA','MAR')
ticker_vol <- NULL
for (ticker in top_10_tickers_2020)
  ticker_vol <- cbind(ticker_vol, getSymbols(ticker, src = "yahoo", from = "2020-01-01", to = "2020-12-31", auto.assign=FALSE)[,5])

drop_na <- apply(ticker_vol,1,function(x) all(!is.na(x)))
ticker_vol <- ticker_vol[drop_na]
colnames(ticker_vol) <- top_10_tickers_2020

corr <- cor(ticker_vol)
ggcorrplot(corr, 
           type = 'upper',
           lab = TRUE,
           lab_size = 3,
           method = 'circle',
           colors = c("red", "white", "green"),
           title="2020",
           legend.title = "Correlation")

```

```{r}
ticker_vol_wsb <- NULL
for (ticker in top_10_tickers)
  ticker_vol_wsb <- cbind(ticker_vol_wsb, getSymbols(ticker, src = "yahoo", from = "2021-01-01", to = "2021-02-01", auto.assign=FALSE)[,5])

drop_na <- apply(ticker_vol_wsb,1,function(x) all(!is.na(x)))
ticker_vol_wsb <- ticker_vol_wsb[drop_na]
colnames(ticker_vol_wsb) <- top_10_tickers

corr_wsb <- cor(ticker_vol_wsb)

ggcorrplot(corr_wsb,
           type = 'upper',
           lab = TRUE,
           lab_size = 3,
           method = 'circle',
           colors = c("red", "white", "green"),
           title="2021 Jan and Feb",
           legend.title = "Correlation")

```


## r/WallStreetBets Reddit Post Data

### r/WallStreetBets metrics

```{r}
reddit_sub <- readr::read_csv("data/clean/reddit/reddit_sub_growth.csv") %>%
  mutate(Date = as.Date(Date, format = "'%Y-%m-%d'"))

reddit_sub %>%
  hchart("line",hcaes(x = Date, y = growth)) %>%
  hc_chart(zoomType = "x") %>%
  hc_colors(c(light_blue)) %>%
  hc_legend(align = "center", verticalAlign = "bottom",layout = "horizontal") %>%
  hc_xAxis(title = list(text = "Date"),
           labels = list(format = '{value:%b %d}')) %>%
  hc_yAxis(title = list(text = "Subscriber growth")) %>%
  hc_title(text = "<b>r/WallStreetBets subscriber growth by Date</b>") %>%
  hc_subtitle(text = "Click and drag in the plot area to zoom in on a time span") %>%
  hc_plotOptions(area = list(lineWidth = 0.5)) %>% 
  hc_exporting(enabled = TRUE)
```

```{r}
reddit_post_ov <- readr::read_csv("data/clean/reddit/wsb_subreddit_overview.csv") %>%
  # select(date_utc, post_cnt, )
  rename(Date = date_utc) %>%
  pivot_longer(!Date, names_to = "attribute", values_to = "value")

reddit_post_ov_ts <- reddit_post_ov %>%
  hchart("line",hcaes(x = Date, y = value, group = attribute)) %>%
  hc_chart(zoomType = "x") %>%
  hc_colors(c(light_blue, light_red, green, purple)) %>%
  hc_legend(align = "center", verticalAlign = "bottom",layout = "horizontal") %>%
  hc_xAxis(title = list(text = "Date"),
           labels = list(format = '{value:%b %d}')) %>%
  hc_yAxis(title = list(text = "Subscriber growth")) %>%
  hc_title(text = "<b>r/WallStreetBets Daily active users and posts count</b>") %>%
  hc_subtitle(text = "Click and drag in the plot area to zoom in on a time span") %>%
  hc_plotOptions(area = list(lineWidth = 0.5)) %>% 
  hc_exporting(enabled = TRUE)


hc_add_annotation(
    reddit_post_ov_ts,
    labelOptions = list(
      backgroundColor = 'rgba(0,0,0,0.4)',
      verticalAlign = 'top',
      y = 15
    ),
    labels = list(
      list(point = list(xAxis = 0, yAxis = 0,
          x = datetime_to_timestamp(as.Date("2021/01/28")),
          y = 170000), text = "First wave"),
      list(point = list(xAxis = 0, yAxis = 0,
          x = datetime_to_timestamp(as.Date("2021/06/03")),
          y = 17000), text = "Second wave")
    )
)
```

### Reddit metrics vs. stock price
```{r}

AMC_2021 <- getSymbols('AMC', src="yahoo", from = "2021-01-01", auto.assign = FALSE)
amc_price_diff <- data.frame(Date=index(AMC_2021),coredata(AMC_2021)) %>%
  select(Date, ends_with("Close")) %>%
  rename(price = ends_with("Close")) %>%
  mutate(price_diff = price - lag(price)) %>%
  select(Date, price_diff)

amc_reddit_post <- readr::read_csv("data/clean/reddit/wsb_subreddit_overview.csv") %>%
  rename(Date = date_utc) %>%
  select(Date, amc_cnt)
  
amc_post_vs_diff <- merge(amc_reddit_post, amc_price_diff) %>%
  drop_na() 

amc_scatter <- ggplot(amc_post_vs_diff, aes(x = amc_cnt, 
                          y = price_diff))+
  geom_point(color = light_red) +
  geom_text(
    aes(label = ifelse(amc_cnt >= 3000 | price_diff>= 10 | price_diff <=-10, as.character(Date), "")),
    hjust=0.5, vjust=0, size=2) +
  labs(x = "# of posts in r/WallStreetBets", 
       y = "daily price change", 
       title = "AMC")+
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        plot.subtitle = element_text(face = "bold", color = "grey35"),
        plot.caption = element_text(color = "grey68"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="bottom",
        legend.title = element_text(face="bold"))
```
```{r}
GME_2021 <- getSymbols('GME', src="yahoo", from = "2021-01-01", auto.assign = FALSE)
gme_price_diff <- data.frame(Date=index(GME_2021),coredata(GME_2021)) %>%
  select(Date, ends_with("Close")) %>%
  rename(price = ends_with("Close")) %>%
  mutate(price_diff = price - lag(price)) %>%
  select(Date, price_diff)

gme_reddit_post <- readr::read_csv("data/clean/reddit/wsb_subreddit_overview.csv") %>%
  rename(Date = date_utc) %>%
  select(Date, gme_cnt)
  
gme_post_vs_diff <- merge(gme_reddit_post, gme_price_diff) %>%
  drop_na() 

gme_scatter <- ggplot(gme_post_vs_diff, aes(x = gme_cnt, 
                          y = price_diff))+
  geom_point(color = light_blue) +
  geom_text(
    aes(label = ifelse(gme_cnt >= 10000 | price_diff >= 50 | price_diff <=-50, as.character(Date), "")),
    hjust=0.5, vjust=0, size=2) +
  labs(x = "# of posts in r/WallStreetBets", 
       y = "daily price change", 
       title = "GME")+
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        plot.subtitle = element_text(face = "bold", color = "grey35"),
        plot.caption = element_text(color = "grey68"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="bottom",
        legend.title = element_text(face="bold"))

amc_gme <- amc_scatter + gme_scatter

amc_gme + plot_annotation(
  title = 'Post count vs daily price change scatterplot',
  subtitle = 'Jan, Feb, May, Jun, July 2021',
)
```


### What are the most popular words?
```{r}
reddit_wc <- readr::read_csv("data/clean/reddit/reddit_wc_total.csv")
wordcloud2(data=reddit_wc, size=0.7, maxRotation = 0, minRotation = 0)
```


### Sentiment analysis on Reddit posts regarding GME
```{r echo=FALSE, message=FALSE}
gme_nrc_first_wave <- readr::read_csv("data/clean/reddit/gme_sa_nrc_first_wave.csv")
gme_nrc_second_wave <- readr::read_csv("data/clean/reddit/gme_sa_nrc_second_wave.csv")
neg_sentiments = c('anger', 'disgust', 'fear', 'negative', 'sadness')
```
```{r}
first_wave <- gme_nrc_first_wave %>%
  select(sentiment, wordCount) %>%
  group_by(sentiment) %>%
  dplyr::summarise(wordCount = sum(wordCount)) %>%
  mutate(label = ifelse(sentiment %in% neg_sentiments, 'neg', 'pos')) %>%
  ggplot(aes(x = fct_reorder(sentiment, desc(wordCount)), y = wordCount)) +
    geom_bar(stat = "identity", aes(fill = label)) +
    scale_fill_manual(values = c("neg" = green,
                                "pos" = light_red)) +
    ggtitle("GME Sentiment Jan and Feb") +
    xlab("sentiment")
second_wave <- gme_nrc_second_wave %>%
  select(sentiment, wordCount) %>%
  group_by(sentiment) %>%
  dplyr::summarise(wordCount = sum(wordCount)) %>%
  mutate(label = ifelse(sentiment %in% neg_sentiments, 'neg', 'pos')) %>%
  ggplot(aes(x = fct_reorder(sentiment, desc(wordCount)), y = wordCount)) +
    geom_bar(stat = "identity", aes(fill = label)) +
    scale_fill_manual(values = c("neg" = green,
                                "pos" = light_red)) +
    ggtitle("GME Sentiment May, Jun and Jul") +
    xlab("sentiment")
first_wave / second_wave
```

```{r}
gme_nrc_total <- rbind(gme_nrc_first_wave, gme_nrc_second_wave)
gme_nrc_sum <- gme_nrc_total %>%
  mutate(label = ifelse(sentiment %in% neg_sentiments, 'neg', 'pos')) %>%
  group_by(textDate, label) %>%
  summarise(wordCount = sum(wordCount)) %>%
  ungroup() %>%
  pivot_wider(id_cols = textDate, names_from = label, values_from = wordCount) %>%
  mutate(positive_percentage = pos/(pos+neg)) %>%
  mutate(positive_percentage = (positive_percentage - min(positive_percentage))/(max(positive_percentage)-min(positive_percentage))*100) %>%
  select(textDate, positive_percentage)
textDate <- seq(as.Date("2021/3/1"), as.Date("2021/5/30"), "days")
empty_df <- data.frame(textDate)
empty_df$positive_percentage <- NA
gme_nrc_sum <- rbind(gme_nrc_sum, empty_df)
gme_stock <- getSymbols("GME", src="yahoo", from = "2021-01-01", to = "2021-07-31", auto.assign = FALSE)
gme_stock_df <- data.frame(Date=index(gme_stock),coredata(gme_stock)) %>% 
  mutate(price = (GME.Close - min(GME.Close))/(max(GME.Close)-min(GME.Close))*100) %>%
  select(Date, price)
total <- merge(gme_stock_df, gme_nrc_sum, by.y="textDate", by.x="Date") %>%
  pivot_longer(!Date, names_to = "attr", values_to = "val")
ggplot(total, aes(x=Date, y=val, color=attr)) +
  geom_line() +
  scale_color_manual(values = c(light_blue, light_red)) +
  ggtitle("GME positive percentage and stock price") +
  ylab("value") +
  labs(color = "") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        plot.caption = element_text(color = "grey68"),
        legend.position="bottom")
```

Above plot analyzes if the sentiments on r/WallStreetBets had any impact on GME's stock price.


### General Public's interests on Google

We believe that the whole event , i.e. r/wallstreetbets subreddit, short squeeze and the associated stocks are relatively new to the public, therefore, they are very likely to search some of the related keywords in Google to understand the whole event. Therefore, in this section, we would like to use the Google search related data to explore the general public attention towards this event. 

#### Interest over time

In Google, people can search in the website and if they already know the basic information about the event and they follow the event closely and only interested in the most recent news, therefore, we would like to zoom one level down into Google News to understand the topics that the close followers are interested in.

We have identified some keywords from the word cloud based on the size of the words there. However, there are some commonly searched word in our daily life, i.e. stock, market, buy and etc, so we need to exclude them from the list. Therefore, we choose `GME`, `AMC`, `wallStreetBet`, `reddit`, `robinhood` to be the key words. However, in the source data, there are misalignment in the word format, i.e. Google data source use `GameStop` to represent `GME`, so we choose the closest word form, which are: `AMC`,`GameStop`, `r/WallStreetBets`,`Reddit`,`Robinhood` respectively. 

Also, since we want to compare the public interest towards these key words before and after the events in order to show how big the effect of the event on the public interest, we choose to display the data during the time period 2020-11-15 to 2021-11-15.

```{r}
combined_web <- readr::read_csv("data/clean/gtrends/combined_web.csv")
combined_news <- readr::read_csv("data/clean/gtrends/combined_news.csv")

web <- combined_web %>%
  ggplot(aes(x=Week, y=score, color = keyword)) +
  geom_line() +
  ggtitle("Web Search Interest overtime")

news <- combined_news %>%
  ggplot(aes(x=Week, y=score, color = keyword)) +
  geom_line() +
  ggtitle("News Search Interest overtime")

web / news
```
In the above 2 graphs, the x-axis is the date. For the y-aixs score, it represents search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.

From the above 2 graphs, we have the following observations:

1. This event has attract huge public attention around the end of Jan, 2021 and start of Feb, 2021 period of time.

2. `GME` caught the highest attention during the 1st wave of the WSB, while `AMC` caught the highest attention during the 2nd wave of the WSB.

3. Generally, the 1st wave of WSB caught more public attention than the 2nd wave of WSB.

4. For the professionals and the closely followed people, more news reported the `GME` and `Robinhood` related topics, since they are the one using `Robinhood` to trade `GME`. While for the general public, they are more care about the different stocks, i.e. `GME` and `AMC`.

5. `Reddit` always has relatively high search interest, which indicates that there are constant active users in `Reddit`, while for `r/WallStreetBets`, before and after the event, only very few people use it. Therefore, the active users in this sub-reddit is not very high, which coincident with our previous conclusion from the daily active user graph.

<!--chapter:end:05-results.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Interactive component

# Interactive component

## Link to the interactive map
Click [here](https://vizhub.com/lulululuxingyu/d413f4871c1449d781395571f0878eec?edit=files&file=stock_change.js) or copy the url: https://vizhub.com/lulululuxingyu/d413f4871c1449d781395571f0878eec?edit=files&file=stock_change.js&mode=full

## Descriptions

The interactive part of our analysis was built with D3 version 6, which the code has been uploaded to [VizHub](https://vizhub.com/). You can also find code [here](https://github.com/stellazhangyue/redditWSB/tree/main/scripts) in our github repo. 
The interactive plot shows the selected 10 stock price related to a highlighted date. Our data source for the plot is [yahoo finance](https://finance.yahoo.com/). You can also find the data [here](https://raw.githubusercontent.com/stellazhangyue/redditWSB/main/data/clean/stock/top_10_stocks.csv) under the `data` folder of our repo.


## Instructions for using the map

The plot is initialized with 10 lines showing the stock price aligned with date Dec 31, 2020. We provide the following interactive options:

1. Users can hover the mouse over the plot to align the price with any date. The y value will become price in a day divided by the price in the highlighted day.  
2. Users can click the legends to select the stocks they want to view. Click twice to bring the stock back.  
3. Users can select volume / price to view.  

Notes: 

- Since the y axis reflects a division, we use log scale to make the plot stable.  

- Once a legend or a variable is clicked, the entire plot will be initialized to align with the first date, Dec 31, 2020.  

References:


1. Observable: Line Chart, Index Chart (https://observablehq.com/@d3/index-chart)  
2. D3 Gallery: Line chart (https://www.d3-graph-gallery.com/line.html)

<!--chapter:end:06-interactive.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Conclusion


<!--chapter:end:07-conclusion.Rmd-->

