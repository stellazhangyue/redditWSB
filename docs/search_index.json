[["index.html", "Analysis of stocks discussed in Reddit r/wallstreetbets Chapter 1 Introduction", " Analysis of stocks discussed in Reddit r/wallstreetbets Xingyu Lu, Yue Xiong ,Yue Zhang 2021-12-12 Chapter 1 Introduction Background: In January, 2021, there is a subreddit called r/wallstreetbets where participants discuss stock and option trading. The participants are mainly young retailer traders who have little background in investment knowledge and risk management. They are not satisfied with the over short on some stocks by major Wall Street companies. In January and June 2021, there were 2 waves of major short squeeze led by the participants in this subreddit, which were GameStop short squeeze and AMC short squeeze. (reference: https://en.wikipedia.org/wiki/R/wallstreetbets) We’ll be trying to answer following questions: 1. How big is the impact of reddit wallstreetbet? I.e. no. of news, downloads of Robinhood and other trading apps, 2. For each targeted stock, what’s the relationship between reddit posts and stock price/volume etc. 3. Why does Wallstreetbet choose these target stocks? What’re their similarities? How do their choices change as time evolves? Gamestop, AMC, Blueberry, Nokia, SPCE, PLTR, TSLA. "],["data-sources.html", "Chapter 2 Data sources", " Chapter 2 Data sources The primary data sources for this project are Yahoo Finance, Reddit API and Google Trends. Yahoo Finance provides financial news, data and commentary including stock quotes, press releases, financial reports, and original content. Reddit API is the official portal Reddit created to facilitate developers in their app-building endeavors, where developers can retrieve the constantly updating feeds of Reddit posts. Google Trends is a website by Google that analyzes the popularity of top search queries in Google Search across various regions and languages. From the above data sources "],["data-transformation.html", "Chapter 3 Data transformation 3.1 Sentiment Analysis on Reddit Posts regarding GME 3.2 Google Trends Data Transformation 3.3 Reddit Data Transformation", " Chapter 3 Data transformation 3.1 Sentiment Analysis on Reddit Posts regarding GME 3.2 Google Trends Data Transformation In the Google Trends source data, it assigns a number to each keyword for each period(week). The numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term. A score of &lt;1 means there is some search interest but it’s extremely low. We are replacing all &lt;1’s with 0.5, so that we can keep the data frame consistent with all scores as numeric data. We also need to tidy the dataset because some of the column names are not names of variables, but values of a variable: the original column names represent the values of the keyword variable and the values in the columns represents the values of score, and each row represents 5 observations, not one. To tidy the dataset, we make the offending columns into a new pair of variables using pivot_longer. After cleaning, it contains 3 rows and 260 records. Column Type Description week date Which week? keyword string Search keyword score numeric Search interest score 3.3 Reddit Data Transformation There were 2 waves of sharp stock price change for WallStreetBets (WSBs) stocks in 2021, which were around January/Feburary and May/June period. These 2 waves are mainly caused by the discussion in the sub-Reddit group, WallStreetBets. Therefore, we have downloaded the Reddit posts that have been posted during these 2 periods and also extend some time before and after the wave. This is because we believe it takes some time for the initial posts to get public attention and then the posts starts to influence the stock price. Also, by including the posts that are slightly after the wave, we can do a comparison during and after the wave to check how big the effects of the posts in the sub-Reddit group on the stock price. Therefore, we have downloaded the data for the whole January, February, May, June and July, which have completely cover the 2 waves and also cover some extension. Also, we have parsed the posts on daily basis, which also helps to show the change over time. Before starting to do the analysis, we need to find the list of WSB stocks first. the Generally speaking, all the stocks that have been discussed in the WallStreetBets sub-Reddit group should be considered as WSB stock. However, given the time frame that we considered is relatively long, which is 5 months, also, the WSBs has caught huge public attention, which also attracted a lot of new redditers. Therefore, there are many people participate in the discussion and the stocks mentioned are quite diversified. In this project, we will only focus on the top 10 stocks that have been mentioned in the posts. In the source data, which are the daily posts csv files, there are 2 columns that may contains the stock information, which are title and selftext respectively. Therefore, we need to first find the stock ticker from these 2 columns. Generally, the stock ticker should start with $ and followed by 3 to 4 consecutive upper case letters. However, in the source data, there is no $ exists, we cannot get the exact stock data but rather, we can extract out the list of possible stock tickers by extracting all the sub-strings which are consists of 3 to 4 consecutive upper case letters. We have used regex in python to extract out the list of possible stock tickers from title and selftext respectively for each of the daily reddit csv and then we combine these 2 lists together. However, there might be the case that the possible stock tickers are mentioned by the redditers both in title and selftext. In this case, we define that the possible stock ticker should be considered only once. Therefore, after combining the 2 lists together, we also need to drop duplicates to make sure there is no double count of possible stock ticker from both title and selftext. With that, we can make sure that the count of every possible stock ticker in 1 row is at most 1. Then, we start to count the possible stock ticker and summarize the count information into a dictionary. For every daily reddit csv, we will get a possible stock ticker count dictionary and combine them together to get the final count dictionary and stored it into the clean data folder. Now, in r, we can read the file which contains the information of the final possible stock ticker count. Since we are only interested in the top 10 WSB stocks, we need to re-order the data frame according to the word count in descending order. We noticed that there are some invalid tickers exists in the data frame, but that is expected since there is no perfect way to extract out the stock ticker due to the lack of $ in the data source, so we manually go through the data frame and select out the top 10 tickers, which are: GME, AMC, NOK, SND, NAK, NAKD, PLTR, CLOV, RETA, MAR, respectively. ## X possible_ticker count ## 1 37 GME 156006 ## 2 42 AMC 85363 ## 3 107 THE 58421 ## 4 62 HOL 37494 ## 5 68 HOLD 36881 ## 6 667 NOK 27053 ## 7 45 WSB 22416 ## 8 78 BUY 21913 ## 9 129 MOO 20864 ## 10 136 MOON 18742 ## 11 128 AND 17170 ## 12 287 YOU 15654 ## 13 103 STO 13745 ## 14 126 FUC 13193 ## 15 135 FUCK 13131 ## 16 210 THI 12894 ## 17 258 NOT 12731 ## 18 458 LIN 12605 ## 19 307 THIS 11520 ## 20 308 LET 10336 ## 21 87 YOL 10289 ## 22 97 YOLO 10269 ## 23 440 SEL 10019 ## 24 2313 DOG 9963 ## 25 124 FOR 9937 ## 26 106 ING 9930 ## 27 442 SELL 9811 ## 28 3187 DOGE 9572 ## 29 570 SND 9426 ## 30 572 SNDL 9378 ## 31 23 KIN 9374 ## 32 238 ALL 8930 ## 33 461 LINE 8365 ## 34 69 ARE 7754 ## 35 158 STOC 7679 ## 36 108 SHO 7549 ## 37 561 CAN 7403 ## 38 63 DIN 7373 ## 39 410 DON 7323 ## 40 919 ROB 6896 ## 41 399 NAK 6815 ## 42 4 MON 6691 ## 43 922 ROBI 6532 ## 44 309 LETS 6243 ## 45 921 OOD 6228 ## 46 920 INH 6211 ## 47 923 NHOO 6175 ## 48 660 NAKD 6139 ## 49 246 SEC 6095 ## 50 291 GET 5897 ## 51 587 CLO 5586 ## 52 115 SHOR 5285 ## 53 90 NOW 5211 ## 54 312 PLT 5076 ## 55 313 PLTR 5040 ## 56 772 THEY 5038 ## 57 142 EVE 5021 ## 58 591 STR 4990 ## 59 109 WIL 4938 ## 60 112 STOP 4929 ## 61 459 TRA 4926 ## 62 25 RET 4851 ## 63 116 WILL 4846 ## 64 480 YOUR 4824 ## 65 120 LIK 4773 ## 66 137 APE 4759 ## 67 131 LIKE 4745 ## 68 2156 CLOV 4712 ## 69 407 ARD 4710 ## 70 212 THA 4691 ## 71 610 EST 4466 ## 72 413 RETA 4441 ## 73 127 DIP 4328 ## 74 904 WIS 4327 ## 75 1862 COIN 4315 ## 76 46 CEO 4304 ## 77 669 MAR 4278 ## 78 70 REA 4274 ## 79 1162 WISH 4258 ## 80 342 RES 4204 ## 81 24 HOD 4183 ## 82 85 CNB 4138 ## 83 86 CNBC 4113 ## 84 263 BOY 4098 ## 85 415 DONT 4093 ## 86 290 HAN 4066 ## 87 162 WIT 4058 ## 88 584 MOR 4056 ## 89 374 WHA 3981 ## 90 169 WITH 3974 ## 91 698 COM 3959 ## 92 240 KET 3910 ## 93 380 WHAT 3878 ## 94 297 HAND 3849 ## 95 344 JUS 3847 ## 96 29 HODL 3841 ## 97 351 JUST 3818 ## 98 146 EVER 3815 ## 99 498 PLE 3787 ## 100 161 SHA 3785 ## 101 61 STI 3783 ## 102 41 DFV 3689 ## 103 655 DIA 3639 ## 104 347 STA 3625 ## 105 462 TRAD 3549 ## 106 375 GOI 3510 ## 107 381 GOIN 3504 ## 108 182 HER 3482 ## 109 76 SQU 3414 ## 110 267 BOYS 3393 ## 111 832 NEW 3361 ## 112 302 GOO 3336 ## 113 598 MORE 3316 ## 114 606 THER 3316 ## 115 168 SHAR 3273 ## 116 296 BUT 3264 ## 117 82 SQUE 3256 ## 118 491 OOO 3253 ## 119 183 APES 3226 ## 120 75 GAM 3221 ## 121 656 DIAM 3209 ## 122 464 SHI 3207 ## 123 233 THAT 3204 ## 124 77 EEZ 3200 ## 125 121 HAV 3179 ## 126 408 SLV 3157 ## 127 213 NEX 3125 ## 128 2039 ECO 3107 ## 129 743 KEE 3093 ## 130 132 HAVE 3085 ## 131 358 ION 3076 ## 132 234 NEXT 3074 ## 133 567 THEM 3058 ## 134 745 KEEP 3043 ## 135 60 DAY 3039 ## 136 604 STRO 3035 ## 137 592 ONG 2981 ## 138 152 TSL 2974 ## 139 153 TSLA 2965 ## 140 257 ITS 2922 ## 141 164 NEE 2838 ## 142 7 FRO 2828 ## 143 252 OUR 2819 ## 144 67 STIL 2799 ## 145 50 IPO 2781 ## 146 816 MAK 2781 ## 147 925 VER 2778 ## 148 59 HEL 2775 ## 149 346 OUT 2758 ## 150 171 NEED 2756 ## 151 634 BAC 2735 ## 152 184 TIME 2712 ## 153 173 GAME 2697 ## 154 325 ONE 2678 ## 155 740 BACK 2678 ## 156 1243 DOW 2667 ## 157 180 TIM 2662 ## 158 594 ETH 2657 ## 159 209 MAN 2596 ## 160 15 FROM 2582 ## 161 2081 DOWN 2577 ## 162 197 INT 2575 ## 163 140 SPC 2554 ## 164 278 EXP 2548 ## 165 141 SPCE 2544 ## 166 670 MARK 2537 ## 167 1458 TAK 2528 ## 168 830 BUYI 2510 ## 169 474 ONL 2479 ## 170 1342 ENT 2478 ## 171 336 TION 2431 ## 172 632 WKH 2430 ## 173 633 WKHS 2427 ## 174 477 ONLY 2406 ## 175 762 PLA 2399 ## 176 1299 BIG 2391 ## 177 329 POS 2389 ## 178 948 MAKE 2389 ## 179 534 WHE 2386 ## 180 596 BRO 2338 ## 181 1362 TLR 2323 ## 182 446 WAL 2315 ## 183 292 TIN 2312 ## 184 2117 TLRY 2312 ## 185 782 STON 2310 ## 186 260 BAB 2302 ## 187 560 GUY 2297 ## 188 905 WOR 2290 ## 189 718 CHA 2279 ## 190 449 WALL 2225 ## 191 57 TOD 2222 ## 192 327 ABO 2218 ## 193 450 STRE 2217 ## 194 310 FIN 2206 ## 195 538 TLD 2206 ## 196 51 COV 2205 ## 197 1688 PRO 2196 ## 198 64 TODA 2195 ## 199 539 TLDR 2191 ## 200 492 MOOO 2178 ## 201 149 TEN 2177 ## 202 694 HERE 2147 ## 203 365 HELP 2102 ## 204 485 WIN 2098 ## 205 13 MONE 2084 ## 206 565 GUYS 2078 ## 207 1683 WTF 2071 ## 208 593 TOG 2061 ## 209 563 HED 2048 ## 210 1545 DIS 2047 ## 211 799 OVE 2043 ## 212 1459 TAKE 2034 ## 213 605 TOGE 2032 ## 214 1167 APP 2026 ## 215 1762 SHIT 2020 ## 216 489 ANY 1985 ## 217 156 CKS 1981 ## 218 2911 NOKI 1979 ## 219 569 HEDG 1968 ## 220 1176 KER 1964 ## 221 2 LEA 1950 ## 222 3294 CLN 1941 ## 223 207 HAS 1938 ## 224 3311 CLNE 1926 ## 225 244 ATE 1924 ## 226 150 DIE 1922 ## 227 1060 VIN 1907 ## 228 1582 BBB 1890 ## 229 1062 LOW 1883 ## 230 759 SPA 1876 ## 231 419 WAN 1870 ## 232 406 NIO 1859 ## 233 144 WHO 1858 ## 234 2916 THES 1850 ## 235 145 LOS 1847 ## 236 391 FUN 1836 ## 237 470 SEN 1834 ## 238 1872 CON 1821 ## 239 1072 TER 1819 ## 240 1180 GOOO 1814 ## 241 1441 SIL 1813 ## 242 1435 XRP 1812 ## 243 788 AGA 1810 ## 244 367 CAL 1809 ## 245 723 LAT 1802 ## 246 789 AGAI 1800 ## 247 1143 END 1798 ## 248 333 ABOU 1797 ## 249 1586 BBBY 1796 ## 250 72 READ 1770 ## 251 1159 BAN 1757 ## 252 732 ERS 1755 ## 253 111 PEN 1750 ## 254 122 RIG 1740 ## 255 1005 INV 1735 ## 256 52 COVI 1728 ## 257 1574 OFF 1726 ## 258 767 NIN 1721 ## 259 451 CTR 1713 ## 260 1269 TOO 1709 ## 261 176 ETF 1685 ## 262 1277 AAL 1685 ## 263 761 SOM 1678 ## 264 668 RED 1665 ## 265 627 TOM 1660 ## 266 265 BABY 1659 ## 267 454 HOW 1659 ## 268 574 WEE 1650 ## 269 497 PEO 1642 ## 270 1442 SILV 1642 ## 271 1858 COI 1637 ## 272 502 PEOP 1635 ## 273 110 HAP 1630 ## 274 452 CTRM 1628 ## 275 704 LOV 1619 ## 276 1595 OPE 1609 ## 277 1339 ASE 1607 ## 278 556 PRI 1605 ## 279 1646 FUND 1605 ## 280 770 SOME 1601 ## 281 1350 PLEA 1598 ## 282 1009 INVE 1596 ## 283 1178 REAL 1596 ## 284 801 OVER 1594 ## 285 159 REM 1592 ## 286 214 WAY 1592 ## 287 2643 UWM 1583 ## 288 43 CCI 1579 ## 289 589 GHT 1576 ## 290 705 LOVE 1575 ## 291 427 WANT 1574 ## 292 1546 TRI 1570 ## 293 259 HIN 1559 ## 294 143 RYO 1555 ## 295 760 SPAC 1553 ## 296 147 YONE 1552 ## 297 261 GOT 1551 ## 298 44 CCIV 1547 ## 299 283 NYS 1542 ## 300 2644 UWMC 1541 ## 301 963 IST 1539 ## 302 1422 INS 1537 ## 303 239 ROC 1532 ## 304 848 FRE 1528 ## 305 368 CALL 1526 ## 306 232 THIN 1525 ## 307 588 BOU 1523 ## 308 852 LIO 1522 ## 309 1542 AUT 1513 ## 310 834 MUS 1512 ## 311 284 NYSE 1508 ## 312 835 SEE 1508 ## 313 503 NAS 1504 ## 314 192 PRE 1502 ## 315 324 WHY 1491 ## 316 270 THAN 1490 ## 317 2856 HOO 1488 ## 318 306 WAR 1487 ## 319 1298 CHE 1474 ## 320 286 RKT 1470 ## 321 493 OOOO 1470 ## 322 444 HCM 1465 ## 323 769 ORT 1464 ## 324 445 HCMC 1458 ## 325 826 RID 1456 ## 326 1503 REV 1455 ## 327 1101 REST 1451 ## 328 215 FIR 1447 ## 329 3092 EXPR 1440 ## 330 628 ORR 1409 ## 331 1033 MOV 1409 ## 332 3634 THEI 1406 ## 333 683 IDE 1392 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 30244 rows ] "],["missing-values.html", "Chapter 4 Missing values 4.1 Reddit post dataset 4.2 Google Trend dataset", " Chapter 4 Missing values 4.1 Reddit post dataset We first create the missing values plots, both count and percent, for the reddit post dataset. We can observe from the missing value plots that there are 4 different missing patterns, which are complete cases (47.19%), missing selftext (45.08%), missing both selftext and author (4.77%), and missing author only (2.95%). Therefore, out of the 7 variables in the dataset, only 2 variables might be missing, which are selftext and author respectively. Among the 2 variables, 49.85% of the selftext is missing while only 7.72% of the author is missing. Looking into the selftext column in the original dataset, we noticed that there are 3 major levels that account for ~90% of the data, which are “removed”, \"\" and “deleted”. Consulted the reddit documents, we found that “removed” means the post is either removed by the moderators of the subreddit group or the administrator; “deleted” means the post is deleted by the content author; \"\" appears when a post doesn’t have body text or a post’s body contains pictures only. In this regard, we are only considering “removed” and “deleted” as NA, \"“s are considered as valid inputs. About 45% of the posts are”removed\", which means the level of censorship is relatively high. ## # A tibble: 95,599 × 3 ## selftext count percent ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 &quot;removed&quot; 444923 44.5 ## 2 &quot;&quot; 408961 40.9 ## 3 &quot;deleted&quot; 46214 4.62 ## 4 &quot;Your daily trading discussion thread Please keep the shitpostin… 87 0.01 ## 5 &quot;Title&quot; 72 0.01 ## 6 &quot;To the moon&quot; 51 0.01 ## 7 &quot;Your daily hype thread Please keep the shitposting to a maximum… 39 0 ## 8 &quot;Your daily trading discussion thread Please keep the shitpostin… 39 0 ## 9 &quot;AMC&quot; 37 0 ## 10 &quot;That is all&quot; 37 0 ## # … with 95,589 more rows Looking at the author column, 7.72% of the posts are missing author. We noticed that 61.76% of the posts that don’t have an author are also missing selftext. ## # A tibble: 7 × 3 ## selftext count percent ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 &lt;NA&gt; 46213 61.7 ## 2 &quot;&quot; 28631 38.2 ## 3 &quot;CROX are the official footwear of going to the moon nothing else … 1 0 ## 4 &quot;I think he s gone and I think he has soled all of this stocks and… 1 0 ## 5 &quot;taken from r investing full credit to OP u MasterCookSwag wanted … 1 0 ## 6 &quot;Welp down 354 today could be worse for sure Just here to say I ha… 1 0 ## 7 &quot;you should absolutely care about it and learn from it Figure out … 1 0 These missing values wouldn’t impact our analysis, because we are looking at the data at the aggregate level. Our analysis only cares about the change of the count of the posts in particular period of time. Even though those posts are deleted or removed in a later time, we should still include them in our analysis. 4.2 Google Trend dataset We created the missing values plots, both count and percent, for two Google trend datasets and found there’s no missing value in both datasets "],["results.html", "Chapter 5 Results 5.1 General Public’s interests on Google 5.2 r/WallStreetBets Reddit Post Data 5.3 Stock Data and Information", " Chapter 5 Results 5.1 General Public’s interests on Google 5.1.1 Interest over time The above two plots display the interest on the relative topics during the time period 2020-11-15 to 2021-11-15. Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term. 5.1.2 Interest by subregion 5.2 r/WallStreetBets Reddit Post Data 5.2.1 Reddit post data overview 5.2.2 Sentiment analysis on Reddit posts regarding GME Above plot analyzes if the sentiments on r/WallStreetBets had any impact on GME’s stock price. 5.3 Stock Data and Information 5.3.1 Stock Data Plot The most Representative WSB stocks are GME and AMC. Also, these 2 stocks are the top 2 most widely discussed stocks in WallStreetBet sub-reddit analysed in Chapter 3 - cleaning. Therefore, we choose these 2 stocks and show the stock price and volume change in the past one year. ## [1] &quot;GME&quot; ## [1] &quot;AMC&quot; From the above 2 graphs, we can clearly see that there are 2 waves of the WSB stocks, which are indicated by the sudden spike of the stock price as well as the sudden increase of the volume. These 2 waves are around January - Feburary and May - June period respectively. These 2 graphs actually double confirm with our previous claims. Also, the gray and red line in the price graph is the Bollinger Bands, which is the price level at 1 standard deviation level above and below the sample moving average of the price, which is able to capture the volatility of the stock. Generally, Bollinger bands is used to check whether prices are relatively high or low as comparing to the historical price. Here, we can see that for the beginning of the 2 waves, the price of both GME and AMC break the Bollinger upper bands, which suggests that the price of these 2 stocks are extremely high as comparing to the historical data. However, the Bollinger bands also adjusted themselves quickly to accommodate the sudden price change, so we can see that the price of these 2 stocks fall back within the Bollinger bands after a short period of time. 5.3.2 Stock Risk Assessment While the previous 2 graphs mainly focus on the price and volume aspects, there is one more aspect that are very important to the stock, which is risk. Risk can be measured by the standard deviation. Here, we would like to explore how the risk of the 2 stocks change before and after the 2 waves, which are GME and AMC respectively. Thus, we need to calculate the moving standard deviation of the 2 stocks over time. For each month, it is about 4 weeks and the stock market only opens in weekdays, thus, there are about 4*5 = 20 days with stock data every month. Therefore, we set the moving window to 20, which is roughly about 1 month time. ## [1] &quot;GME&quot; "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
